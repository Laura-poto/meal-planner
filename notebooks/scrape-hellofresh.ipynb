{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b4fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e19b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] https://www.hellofresh.fr/recipes/tatin-doignons-and-burrata-65203d49188676f47c1b3086 ajouté\n",
      "Fichier écrit: ../data/recettes_hellofresh.txt\n",
      "240 recettes sauvegardées\n"
     ]
    }
   ],
   "source": [
    "# SCRAPING RECETTES HELLOFRESH → + desc_part_1..6 + total_time, prep_time, difficulty\n",
    "import json, re, time, math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "UA = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "UNITS = [\n",
    "    \"g\",\"kg\",\"mg\",\"ml\",\"cl\",\"l\",\n",
    "    \"cs\",\"cc\",\"c.à.s\",\"c.à.c\",\n",
    "    \"sachet\",\"sachet(s)\",\n",
    "    \"pièce\",\"pièce(s)\",\n",
    "    \"tranche\",\"tranche(s)\",\n",
    "    \"botte\",\"botte(s)\",\n",
    "    \"pincée\",\"pincée(s)\",\n",
    "    \"brin\",\"brin(s)\",\n",
    "    \"bouquet\",\"bouquet(s)\",\n",
    "    \"paquet\",\"paquet(s)\",\n",
    "    \"pot\",\"pot(s)\",\n",
    "    \"cm\",\n",
    "    \"boule\", \"boule(s)\",\n",
    "    \"boîte(s)\", \"boîte\",\n",
    "    \"filet(s)\",\n",
    "    \"tube(s)\", \"tube\"\n",
    "]\n",
    "TEXT_QTY = [\"selon le goût\", \"au goût\", \"à volonté\"]\n",
    "\n",
    "FRACTION_MAP = {\n",
    "    \"¼\": 1/4, \"½\": 1/2, \"¾\": 3/4,\n",
    "    \"⅐\": 1/7, \"⅑\": 1/9, \"⅒\": 1/10,\n",
    "    \"⅓\": 1/3, \"⅔\": 2/3,\n",
    "    \"⅕\": 1/5, \"⅖\": 2/5, \"⅗\": 3/5, \"⅘\": 4/5,\n",
    "    \"⅙\": 1/6, \"⅚\": 5/6,\n",
    "    \"ⅈ\": None,  # rare, laissé ici au cas où\n",
    "    \"⅛\": 1/8, \"⅜\": 3/8, \"⅝\": 5/8, \"⅞\": 7/8,\n",
    "}\n",
    "FRACTIONS_CLASS = \"\".join(k for k,v in FRACTION_MAP.items() if v is not None)\n",
    "NUM = rf\"(?:\\d+(?:[.,]\\d+)?|\\d+/\\d+|[{re.escape(FRACTIONS_CLASS)}])\"\n",
    "\n",
    "UNIT = r\"(?:{})(?!\\S)\".format(\"|\".join([re.escape(u) for u in UNITS]))\n",
    "QTY_CORE = rf\"{NUM}(?:\\s*{UNIT})?\"\n",
    "LEADING_QTY_RE = re.compile(rf\"^\\s*({QTY_CORE}(?:\\s+{QTY_CORE})*)\\s+(.+?)\\s*$\")\n",
    "TRAILING_QTY_RE = re.compile(rf\"^\\s*(.+?)\\s+({QTY_CORE}(?:\\s+{QTY_CORE})*)\\s*$\")\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip())\n",
    "\n",
    "def frac_to_float(s: str):\n",
    "    s = s.strip().replace(\",\", \".\")\n",
    "    if s in FRACTION_MAP and FRACTION_MAP[s] is not None:\n",
    "        return FRACTION_MAP[s]\n",
    "    if \"/\" in s:\n",
    "        try:\n",
    "            a, b = s.split(\"/\")\n",
    "            return float(a) / float(b)\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def split_qty_name(line: str):\n",
    "    s = normalize_space(line)\n",
    "    for t in TEXT_QTY:\n",
    "        if s.lower().startswith(t):\n",
    "            rem = normalize_space(s[len(t):])\n",
    "            name = rem if rem else s\n",
    "            return t, name\n",
    "    m = LEADING_QTY_RE.match(s)\n",
    "    if m:\n",
    "        qty_text = normalize_space(m.group(1))\n",
    "        name = normalize_space(m.group(2))\n",
    "        return qty_text, name\n",
    "    m = TRAILING_QTY_RE.match(s)\n",
    "    if m:\n",
    "        name = normalize_space(m.group(1))\n",
    "        qty_text = normalize_space(m.group(2))\n",
    "        return qty_text, name\n",
    "    return None, s\n",
    "\n",
    "def split_number_and_unit(qty_text: str):\n",
    "    if qty_text is None:\n",
    "        return None, \"\"\n",
    "    t = qty_text.strip()\n",
    "    for txt in TEXT_QTY:\n",
    "        if t.lower() == txt:\n",
    "            return None, txt\n",
    "    m = re.match(rf\"^\\s*({NUM})(?:\\s+(.+))?\\s*$\", t)\n",
    "    if not m:\n",
    "        return None, t\n",
    "    n_raw = m.group(1)\n",
    "    unit = (m.group(2) or \"\").strip()\n",
    "    n = frac_to_float(n_raw)\n",
    "    if n is not None:\n",
    "        if abs(n - round(n)) < 1e-9:\n",
    "            n = int(round(n))\n",
    "        else:\n",
    "            n = math.floor(n * 100 + 1e-9) / 100.0\n",
    "    return n, unit\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    return re.sub(r\"\\s*Recette\\s*\\|\\s*HelloFresh\\s*$\", \"\", title).strip()\n",
    "\n",
    "# ---- Instructions\n",
    "def get_instructions(soup: BeautifulSoup):\n",
    "    steps = []\n",
    "    for box in soup.find_all(attrs={\"data-test-id\": \"instruction-step\"}):\n",
    "        items = [li.get_text(\" \", strip=True) for li in box.find_all(\"li\")]\n",
    "        if items:\n",
    "            steps.append(\" \".join(items))\n",
    "            continue\n",
    "        txt = box.get_text(\" \", strip=True)\n",
    "        if txt:\n",
    "            steps.append(txt)\n",
    "    if steps:\n",
    "        return steps\n",
    "    header = None\n",
    "    for h in soup.find_all([\"h2\", \"h3\", \"h4\"]):\n",
    "        if h.get_text(strip=True).lower().startswith(\"instructions\"):\n",
    "            header = h\n",
    "            break\n",
    "    if header:\n",
    "        section_steps = []\n",
    "        for sib in header.parent.next_siblings:\n",
    "            name = getattr(sib, \"name\", None)\n",
    "            if name in (\"h2\", \"h3\", \"h4\"):\n",
    "                break\n",
    "            if hasattr(sib, \"find_all\"):\n",
    "                for ul in sib.find_all(\"ul\"):\n",
    "                    items = [li.get_text(\" \", strip=True) for li in ul.find_all(\"li\")]\n",
    "                    if items:\n",
    "                        section_steps.append(\" \".join(items))\n",
    "        if section_steps:\n",
    "            return section_steps\n",
    "    return steps\n",
    "\n",
    "# ---- Durées & difficulté\n",
    "ISO_RE = re.compile(r\"^P(T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?)$\", re.I)\n",
    "\n",
    "def iso_to_human(iso: str) -> str:\n",
    "    \"\"\"\n",
    "    'PT40M' -> '40 minutes', 'PT1H30M' -> '1 h 30 min'\n",
    "    \"\"\"\n",
    "    if not iso:\n",
    "        return \"\"\n",
    "    m = ISO_RE.match(iso.strip())\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    h = int(m.group(2) or 0)\n",
    "    mnt = int(m.group(3) or 0)\n",
    "    parts = []\n",
    "    if h:\n",
    "        parts.append(f\"{h} h\")\n",
    "    if mnt:\n",
    "        parts.append(f\"{mnt} min\" if not h else f\"{mnt} min\")\n",
    "    return \" \".join(parts) if parts else \"\"\n",
    "\n",
    "# --- Precise DOM fallback for 'Temps total' / 'Temps de préparation' / 'Difficulté'\n",
    "\n",
    "def _value_after_label(soup: BeautifulSoup, label: str, want_digits=False):\n",
    "    \"\"\"\n",
    "    Find the node whose text == label, then scan the next few spans/divs\n",
    "    for the first non-empty value (preferring one with digits when want_digits=True).\n",
    "    \"\"\"\n",
    "    n = soup.find(string=re.compile(rf\"^\\s*{re.escape(label)}\\s*$\", re.I))\n",
    "    if not n:\n",
    "        return \"\"\n",
    "\n",
    "    # search nearby first (same row / immediate siblings), then a short forward scan\n",
    "    anchors = []\n",
    "    # parent chain for local search\n",
    "    p = getattr(n, \"parent\", None)\n",
    "    for _ in range(4):  # climb a few levels max\n",
    "        if not p:\n",
    "            break\n",
    "        anchors.append(p)\n",
    "        p = getattr(p, \"parent\", None)\n",
    "\n",
    "    # candidates: spans/divs under those anchors (row) and a short forward walk\n",
    "    seen_ids = set()\n",
    "    cands = []\n",
    "    for a in anchors:\n",
    "        for el in a.find_all([\"span\", \"div\"], recursive=True):\n",
    "            if id(el) in seen_ids: \n",
    "                continue\n",
    "            seen_ids.add(id(el))\n",
    "            cands.append(el)\n",
    "\n",
    "    # also scan forward a bit in document order\n",
    "    for el in n.find_all_next([\"span\", \"div\"], limit=12):\n",
    "        if id(el) in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(id(el))\n",
    "        cands.append(el)\n",
    "\n",
    "    # choose first usable text that is not the label itself\n",
    "    for el in cands:\n",
    "        t = el.get_text(\" \", strip=True)\n",
    "        if not t:\n",
    "            continue\n",
    "        if re.fullmatch(rf\"\\s*{re.escape(label)}\\s*\", t, flags=re.I):\n",
    "            continue\n",
    "        if want_digits and not re.search(r\"\\d\", t):\n",
    "            continue\n",
    "        return t\n",
    "    return \"\"\n",
    "\n",
    "def extract_meta_from_dom(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    DOM-driven fallback: pick the value next to the label.\n",
    "    \"\"\"\n",
    "    total = _value_after_label(soup, \"Temps total\", want_digits=True)\n",
    "    prep  = _value_after_label(soup, \"Temps de préparation\", want_digits=True)\n",
    "    diff  = _value_after_label(soup, \"Difficulté\", want_digits=False)\n",
    "    return total, prep, diff\n",
    "\n",
    "def parse_recipe_jsonld_only(url: str):\n",
    "    r = requests.get(url, headers=UA, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Titre\n",
    "    title = None\n",
    "    og = soup.find(attrs={\"property\": \"og:title\"})\n",
    "    if og and og.get(\"content\"):\n",
    "        title = clean_title(og[\"content\"])\n",
    "    if not title:\n",
    "        h1 = soup.find(\"h1\")\n",
    "        title = clean_title(h1.get_text(strip=True) if h1 else \"Recette\")\n",
    "\n",
    "    ingredients_obj = {}\n",
    "    found_any = False\n",
    "    total_time = \"\"\n",
    "    prep_time = \"\"\n",
    "    difficulty = \"\"\n",
    "\n",
    "    # JSON-LD\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "        except Exception:\n",
    "            continue\n",
    "        objs = data if isinstance(data, list) else [data]\n",
    "        for obj in objs:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            t = obj.get(\"@type\")\n",
    "            if (isinstance(t, list) and \"Recipe\" in t) or t == \"Recipe\":\n",
    "                # 1) Times via JSON-LD si présents\n",
    "                if not total_time:\n",
    "                    total_time = iso_to_human(obj.get(\"totalTime\", \"\"))\n",
    "                if not prep_time:\n",
    "                    prep_time = iso_to_human(obj.get(\"prepTime\", \"\"))\n",
    "                # parfois difficulté n’existe pas en JSON-LD\n",
    "\n",
    "                # 2) Ingrédients\n",
    "                for raw in (obj.get(\"recipeIngredient\") or []):\n",
    "                    qty_text, name = split_qty_name(str(raw))\n",
    "                    qty_num, unit = split_number_and_unit(qty_text)\n",
    "                    if name:\n",
    "                        ingredients_obj[name] = {\"qty\": qty_num, \"unit\": unit or \"\"}\n",
    "                        found_any = True\n",
    "    # Fallback texte/DOM pour time/difficulty si manquants\n",
    "    if not total_time or not prep_time or not difficulty:\n",
    "        t_fallback, p_fallback, d_fallback = extract_meta_from_dom(soup)\n",
    "        if not total_time and t_fallback:\n",
    "            total_time = t_fallback\n",
    "        if not prep_time and p_fallback:\n",
    "            prep_time = p_fallback\n",
    "        if not difficulty and d_fallback:\n",
    "            difficulty = d_fallback\n",
    "\n",
    "    # Instructions\n",
    "    steps = get_instructions(soup)\n",
    "    desc = {}\n",
    "    for i in range(1, 7):\n",
    "        if i < 6:\n",
    "            desc[f\"desc_part_{i}\"] = steps[i-1] if i-1 < len(steps) else \"\"\n",
    "        else:\n",
    "            desc[f\"desc_part_{i}\"] = \" \".join(steps[5:]) if len(steps) > 5 else (steps[5] if len(steps) > 5 else \"\")\n",
    "\n",
    "    return {\n",
    "        \"name\": title,\n",
    "        \"link\": url,\n",
    "        \"ingredients\": ingredients_obj if found_any else {},\n",
    "        \"total_time\": total_time,         # ex. '40 minutes' ou '1 h 30 min'\n",
    "        \"prep_time\": prep_time,           # ex. '35 minutes'\n",
    "        \"difficulty\": difficulty,         # ex. 'Intermédiaire'\n",
    "        **desc\n",
    "    }\n",
    "\n",
    "def scrape_many_to_file(urls, out_path=\"../data/recettes_hellofresh.txt\"):\n",
    "    existing = {}\n",
    "    out_file = Path(out_path)\n",
    "    if out_file.exists():\n",
    "        try:\n",
    "            with open(out_file, encoding=\"utf-8\") as f:\n",
    "                old_data = json.load(f)\n",
    "                for rec in old_data:\n",
    "                    if rec and rec.get(\"link\"):\n",
    "                        existing[rec[\"link\"]] = rec\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Impossible de charger l’existant ({e})\")\n",
    "\n",
    "    results = dict(existing)\n",
    "    seen = set()\n",
    "\n",
    "    for u in urls:\n",
    "        url = u.strip().rstrip(\"/\")\n",
    "        if not url or url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "\n",
    "        if url in existing:\n",
    "            print(f\"[skip] {url} déjà présent, on garde l’ancien\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rec = parse_recipe_jsonld_only(url)\n",
    "            results[url] = rec\n",
    "            print(f\"[ok] {url} ajouté\")\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] {url}: {e}\")\n",
    "            if url not in results:\n",
    "                results[url] = {\"name\": None, \"link\": url, \"ingredients\": {}}\n",
    "\n",
    "    final_list = list(results.values())\n",
    "    out_file.write_text(json.dumps(final_list, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_path, final_list\n",
    "\n",
    "# --- Exemple d'utilisation \n",
    "# --- # Charger les URLs depuis un fichier JSON (.txt) \n",
    "with open(\"../data/urls_hellofresh.txt\", encoding=\"utf-8\") as f: \n",
    "    urls = json.load(f) \n",
    "out_file, data = scrape_many_to_file(urls) \n",
    "print(\"Fichier écrit:\", out_file) \n",
    "print(f\"{len(data)} recettes sauvegardées\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
